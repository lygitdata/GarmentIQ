{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTYHtjS_NzhK"
   },
   "source": [
    "# GarmentIQ Classification Model Fine-tuning - Advanced Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7eOzqrGPvLp"
   },
   "outputs": [],
   "source": [
    "import garmentiq as giq\n",
    "from garmentiq.classification.model_definition import tinyViT\n",
    "from garmentiq.classification.utils import CachedDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIb-L3XU9KJn"
   },
   "outputs": [],
   "source": [
    "# Download fine-tuning data\n",
    "# To train a model using GarmentIQ framework, your data must be in a zip file\n",
    "# and the zip file should have the same structure as our data. See the link:\n",
    "# https://www.kaggle.com/datasets/lygitdata/zara-clothes-image-data\n",
    "!curl -L -o /app/working/examples/classification_model_fine_tuning_advanced_usage/zara-clothes-image-data.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/lygitdata/zara-clothes-image-data\n",
    "\n",
    "# Download the base model - tinyViT - to be finetuned\n",
    "!mkdir -p /app/working/examples/classification_model_fine_tuning_advanced_usage/models\n",
    "!wget -q -O /app/working/examples/classification_model_fine_tuning_advanced_usage/models/tiny_vit.pt \\\n",
    "    https://huggingface.co/lygitdata/garmentiq/resolve/main/tiny_vit.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "col7KU5vOFt6"
   },
   "outputs": [],
   "source": [
    "# Prepare the data for fine-tuning\n",
    "# As our data size is small, we make the testing set to be 0%\n",
    "# You can see from the test set summary that the size is 0\n",
    "data = giq.classification.train_test_split(\n",
    "    output_dir=\"/app/working/examples/classification_model_fine_tuning_advanced_usage/data\",\n",
    "    train_zip_dir=\"/app/working/examples/classification_model_fine_tuning_advanced_usage/zara-clothes-image-data.zip\",\n",
    "    metadata_csv=\"metadata.csv\",\n",
    "    label_column=\"garment\",\n",
    "    test_size=0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcS54pI1QrJx"
   },
   "outputs": [],
   "source": [
    "# Load the training set into memory for faster I/O during training\n",
    "train_images, train_labels, _ = giq.classification.load_data(\n",
    "    df=data[\"train_metadata\"],\n",
    "    img_dir=data[\"train_images\"],\n",
    "    label_column=\"garment\",\n",
    "    resize_dim=(120, 184),\n",
    "    normalize_mean=[0.8047, 0.7808, 0.7769],\n",
    "    normalize_std=[0.2957, 0.3077, 0.3081]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTfBwce_IH1r"
   },
   "outputs": [],
   "source": [
    "# Fine-tune the pretrained tinyViT model\n",
    "# For demonstration purpose, we only use 5 folds and 5 epochs\n",
    "# Models are saved at the folder `finetuned_models`\n",
    "# It automatically selects the model with the lowest cross entropy\n",
    "# as the best model\n",
    "giq.classification.fine_tune_pytorch_nn(\n",
    "    model_class=tinyViT,\n",
    "    model_args={\"num_classes\": 9, \"img_size\": (120, 184), \"patch_size\": 6},\n",
    "    dataset_class=CachedDataset,\n",
    "    dataset_args={\n",
    "        \"metadata_df\": data[\"train_metadata\"],\n",
    "        \"raw_labels\": data[\"train_metadata\"][\"garment\"],\n",
    "        \"cached_images\": train_images,\n",
    "        \"cached_labels\": train_labels,\n",
    "    },\n",
    "    param={\n",
    "        \"pretrained_path\": \"/app/working/examples/classification_model_fine_tuning_advanced_usage/models/tiny_vit.pt\",\n",
    "        \"freeze_layers\": True,\n",
    "        \"unfreeze_patterns\": [\"classifier\", \"fc\"],\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_args\": {\"lr\": 0.00002, \"weight_decay\": 1e-4},\n",
    "        \"n_fold\": 5,\n",
    "        \"n_epoch\": 5,\n",
    "        \"patience\": 2,\n",
    "        \"batch_size\": 128,\n",
    "        \"model_save_dir\": \"/app/working/examples/classification_model_fine_tuning_advanced_usage/finetuned_models\",\n",
    "        \"best_model_name\": \"best_finetuned.pt\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9y2RIbsJenJ"
   },
   "outputs": [],
   "source": [
    "# See the performance of the finetuned model on the fine-tuning dataset\n",
    "giq.classification.test_pytorch_nn(\n",
    "    model_path=\"/app/working/examples/classification_model_fine_tuning_advanced_usage/finetuned_models/best_finetuned.pt\",\n",
    "    model_class=tinyViT,\n",
    "    model_args={\"num_classes\": 9, \"img_size\": (120, 184), \"patch_size\": 6},\n",
    "    dataset_class=CachedDataset,\n",
    "    dataset_args={\n",
    "        \"raw_labels\": data[\"train_metadata\"][\"garment\"],\n",
    "        \"cached_images\": train_images,\n",
    "        \"cached_labels\": train_labels,\n",
    "    },\n",
    "    param={\"batch_size\": 64},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBgI9OQorOESyY4wReaUl+",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
