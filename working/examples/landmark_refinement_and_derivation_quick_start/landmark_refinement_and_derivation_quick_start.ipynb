{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrn-fD7chMUe"
   },
   "source": [
    "# GarmentIQ Landmark Detection Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8RxkKlsh5IY"
   },
   "outputs": [],
   "source": [
    "import garmentiq as giq\n",
    "from garmentiq.landmark.detection.model_definition import PoseHighResolutionNet\n",
    "from garmentiq.garment_classes import garment_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-b9AZr9hJyK"
   },
   "outputs": [],
   "source": [
    "# Download a vest dress image and a pretrained model\n",
    "!mkdir -p test_image\n",
    "!wget -q -O /app/working/examples/landmark_refinement_and_derivation_quick_start/test_image/cloth_3.jpg \\\n",
    "    https://raw.githubusercontent.com/lygitdata/GarmentIQ/refs/heads/gh-pages/asset/img/cloth_3.jpg\n",
    "\n",
    "!mkdir -p models\n",
    "!wget -q -O /app/working/examples/landmark_refinement_and_derivation_quick_start/models/hrnet.pth \\\n",
    "    https://huggingface.co/lygitdata/garmentiq/resolve/main/hrnet.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYFrZUe48hLi"
   },
   "outputs": [],
   "source": [
    "# Plot the image\n",
    "giq.landmark.plot(\n",
    "    image_path=\"/app/working/examples/landmark_refinement_and_derivation_quick_start/test_image/cloth_3.jpg\", \n",
    "    figsize=(3, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMGZwL-UjPPN"
   },
   "outputs": [],
   "source": [
    "# Load the pretrained model from Hugging Face\n",
    "HRNet = giq.landmark.detection.load_model(\n",
    "    model_path=\"/app/working/examples/landmark_refinement_and_derivation_quick_start/models/hrnet.pth\",\n",
    "    model_class=PoseHighResolutionNet()\n",
    ")\n",
    "\n",
    "# Detect predefined landmarks\n",
    "coords, maxvals, detection_dict = giq.landmark.detect(\n",
    "    class_name=\"vest dress\",\n",
    "    class_dict=garment_classes,\n",
    "    image_path=\"/app/working/examples/landmark_refinement_and_derivation_quick_start/test_image/cloth_3.jpg\",\n",
    "    model=HRNet,\n",
    "    scale_std=200.0,\n",
    "    resize_dim=[288, 384],\n",
    "    normalize_mean=[0.485, 0.456, 0.406],\n",
    "    normalize_std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Plot the detected coordinates\n",
    "giq.landmark.plot(\n",
    "    image_path=\"/app/working/examples/landmark_refinement_and_derivation_quick_start/test_image/cloth_3.jpg\", \n",
    "    coordinate=coords, \n",
    "    figsize=(3, 3), \n",
    "    color=\"green\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4-zd5CusfED"
   },
   "outputs": [],
   "source": [
    "# Segmentation mask is required for refinement and derivation\n",
    "# So we need to do segmentation first\n",
    "BiRefNet = giq.segmentation.load_model(\n",
    "    pretrained_model='lygitdata/BiRefNet_garmentiq_backup',\n",
    "    pretrained_model_args={'trust_remote_code': True},\n",
    "    high_precision=True\n",
    ")\n",
    "original_img, mask = giq.segmentation.extract(\n",
    "    model=BiRefNet,\n",
    "    image_path='/app/working/examples/landmark_refinement_and_derivation_quick_start/test_image/cloth_3.jpg',\n",
    "    resize_dim=(1024, 1024),\n",
    "    normalize_mean=[0.485, 0.456, 0.406],\n",
    "    normalize_std=[0.229, 0.224, 0.225],\n",
    "    high_precision=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deJChHYZ7rpT"
   },
   "outputs": [],
   "source": [
    "# Refine the landmarks\n",
    "refined_coords, refined_detection_dict = giq.landmark.refine(\n",
    "    class_name=\"vest dress\",\n",
    "    detection_np=coords,\n",
    "    detection_conf=maxvals,\n",
    "    detection_dict=detection_dict,\n",
    "    mask=mask,\n",
    "    window_size=5,\n",
    "    ksize=(11, 11),\n",
    "    sigmaX=0.0\n",
    ")\n",
    "\n",
    "# Print the original coordinates and the refined coordinates\n",
    "print(\"Original coordinates:\\n\", coords)\n",
    "print(\"Refined coordinates:\\n\", refined_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpKL-_2685Rd"
   },
   "outputs": [],
   "source": [
    "# Derive custom landmarks\n",
    "derived_coords, derived_detection_dict = giq.landmark.derive(\n",
    "    class_name=\"vest dress\",\n",
    "    detection_dict=refined_detection_dict,\n",
    "    derivation_dict=giq.landmark.derivation_dict.derivation_dict,\n",
    "    landmark_coords=refined_coords,\n",
    "    np_mask=mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTskt_1KwmBG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plot the derived point\n",
    "giq.landmark.plot(\n",
    "    image_path=\"/app/working/examples/landmark_refinement_and_derivation_quick_start/test_image/cloth_3.jpg\",\n",
    "    coordinate=np.concatenate((refined_coords, np.array([[derived_coords['20']]])), axis=1),\n",
    "    figsize=(3, 3),\n",
    "    color=\"green\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
